{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "CS 470 Assignment 1\n",
    "Implementing a Multi-layer Perceptron (MLP)\n",
    "In this exercise, you will implement a neural network with fully-connected layers to perform image classification, and test it out on the CIFAR-10 dataset. Please, run following blocks for running your code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "  \"\"\" load single batch of cifar \"\"\"\n",
    "  with open(filename, 'rb') as f:\n",
    "    datadict = pickle.load(f, encoding='latin1')\n",
    "    X = datadict['data']\n",
    "    Y = datadict['labels']\n",
    "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "  \"\"\" load all of cifar \"\"\"\n",
    "  xs = []\n",
    "  ys = []\n",
    "  for b in range(1,6):\n",
    "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "    X, Y = load_CIFAR_batch(f)\n",
    "    xs.append(X)\n",
    "    ys.append(Y)\n",
    "  Xtr = np.concatenate(xs)\n",
    "  Ytr = np.concatenate(ys)\n",
    "  del X, Y\n",
    "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "  return Xtr, Ytr, Xte, Yte"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3072)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the MLP classifier.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape the data\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this work, we use a neural network with fully-connected layers in which each layer is represented as a function:  ùëì(ùê±)=ùêñùê±+ùêõ , where  ùêñ  is a weight matrix,  ùê±  is its input, and  ùêõ  is a bias. The activation function of the first layer is the ReLU function:  ùúé(ùê±)=max(0,ùê±) ."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1./(1.+np.exp(-x))\n",
    "\n",
    "class MLP(object):\n",
    "  \"\"\"\n",
    "  A multi-layer fully-connected neural network has an input dimension of\n",
    "  d, a hidden layer dimension of h, and performs classification over c classes.\n",
    "  You must train the network with a softmax loss function and L1 regularization on the\n",
    "  weight matrices. The network uses a ReLU/LeakyReLU/etc nonlinearity after the first fully\n",
    "  -connected layer.\n",
    "\n",
    "  The network has the following architecture:\n",
    "\n",
    "  Input - Linear layer - ReLU/LeakyReLU/etc - Linear layer - Softmax\n",
    "\n",
    "  The outputs of the network are the labels for each class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, output_size, activation, std=1e-4):\n",
    "    \"\"\"\n",
    "    An initialization function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size: integer\n",
    "        the dimension d of the input data.\n",
    "    hidden_size: integer\n",
    "        the number of neurons h in the hidden layer.\n",
    "    output_size: integer\n",
    "        the number of classes c.\n",
    "    activation: string\n",
    "        activation method name\n",
    "    std: float\n",
    "        standard deviation\n",
    "    \"\"\"\n",
    "    # w1: weights for the first linear layer\n",
    "    # b1: biases for the first linear layer\n",
    "    # w2: weights for the second linear layer\n",
    "    # b2: biases for the second linear layer\n",
    "\n",
    "    self.params = {}\n",
    "    self.params['w1'] = std * np.random.randn(input_size, hidden_size)\n",
    "    self.params['b1'] = np.zeros(hidden_size)\n",
    "    self.params['w2'] = std * np.random.randn(hidden_size, output_size)\n",
    "    self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    self.leaky_relu_c = 0.01\n",
    "    self.selu_lambda  = 1.05\n",
    "    self.selu_alpha   = 1.67\n",
    "    self.activation_method = ['ReLU','LeakyReLU','SWISH','SELU'].index(activation)\n",
    "    print(\"Selected using \"+['ReLU','LeakyReLU','SWISH','SELU'][self.activation_method])\n",
    "\n",
    "\n",
    "  def forward_pass(self, x, w1, b1, w2, b2):\n",
    "    \"\"\"\n",
    "    A forward pass function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out:\n",
    "        network output\n",
    "    cache:\n",
    "        intermediate values\n",
    "    \"\"\"\n",
    "    h1     = None  # the activation after the first linear layer\n",
    "    y1, y2 = None, None  # outputs from the first & second linear layers\n",
    "\n",
    "    #############################################################################\n",
    "    # PLACE YOUR CODE HERE                                                      #\n",
    "    #############################################################################\n",
    "    # TODO: Design the fully-connected neural network and compute its forward\n",
    "    #       pass output,\n",
    "    #        Input - Linear layer - LeakyReLU - Linear layer.\n",
    "    #       You have use predefined variables above\n",
    "\n",
    "    #  START OF YOUR CODE\n",
    "    y1 = np.dot(x, w1) + b1\n",
    "\n",
    "    if self.activation_method == 0:\n",
    "      # ReLU\n",
    "      h1 = np.maximum(0, y1)\n",
    "    elif self.activation_method == 1:\n",
    "      # Leaky ReLU\n",
    "      h1 = np.maximum(self.leaky_relu_c * y1, y1)\n",
    "    elif self.activation_method == 2:\n",
    "      # SWISH\n",
    "      h1 = y1 * sigmoid(y1)\n",
    "    else:\n",
    "      # SELU\n",
    "      h1 = self.selu_lambda * np.maximum(0, y1) + self.selu_alpha * (np.exp(np.minimum(0, y1)) - 1)\n",
    "\n",
    "    y2 = np.dot(h1, w2) + b2\n",
    "\n",
    "    #  END OF YOUR CODE\n",
    "    #############################################################################\n",
    "\n",
    "    out   = y2\n",
    "    cache = (y1, h1) # intermediate values\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "  def softmax_loss(self, x, y):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradients for a softmax classifier\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss:\n",
    "        the softmax loss\n",
    "    dx:\n",
    "        the gradient of loss\n",
    "\n",
    "    \"\"\"\n",
    "    #############################################################################\n",
    "    # PLACE YOUR CODE HERE                                                      #\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax classification loss and its gradient.           #\n",
    "    # The softmax loss is also known as cross-entropy loss.                     #\n",
    "\n",
    "    #implement cross entropy loss below\n",
    "\n",
    "    loss = None\n",
    "    dx = None\n",
    "\n",
    "\n",
    "    #  END OF YOUR CODE\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, dx\n",
    "\n",
    "\n",
    "  def backward_pass(self, dY2_dLoss, x, w1, y1, h1, w2):\n",
    "    \"\"\"\n",
    "    A backward pass function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grads:\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "\n",
    "    #############################################################################\n",
    "    # PLACE YOUR CODE HERE                                                      #\n",
    "    #############################################################################\n",
    "    # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
    "    # and biases. Store the results in the grads dictionary. For example,       #\n",
    "    # the gradient on W1 should be stored in grads['w1'] and be a matrix of same#\n",
    "    # size                                                                      #\n",
    "\n",
    "    #without regularization\n",
    "    #grads['w2'] =\n",
    "    #grads['b2'] =\n",
    "\n",
    "    if self.activation_method == 0:\n",
    "      # ReLU\n",
    "      pass\n",
    "\n",
    "      #dY1_dLoss =\n",
    "    elif self.activation_method == 1:\n",
    "      # Leaky ReLU\n",
    "      pass\n",
    "\n",
    "      #dY1_dLoss =\n",
    "    elif self.activation_method == 2:\n",
    "      # SWISH\n",
    "      pass\n",
    "\n",
    "      #dY1_dLoss =\n",
    "    else:\n",
    "      # SELU\n",
    "      pass\n",
    "\n",
    "     #dY1_dLoss =\n",
    "\n",
    "    #grads['w1'] =\n",
    "    #grads['b1'] =\n",
    "\n",
    "    #  END OF YOUR CODE\n",
    "    #############################################################################\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "  def loss(self, x, y=None, regular=0.0, enable_margin=False):\n",
    "    \"\"\"\n",
    "    A loss function that returns the loss and gradients of the fully-connected\n",
    "    neural network. This function requires designing forward and backward passes.\n",
    "\n",
    "    If y is None, it returns a matrix labelsof shape (n, c) where labels[i, c]\n",
    "    is the label score for class c on input x[i]. Otherwise, it returns a tuple\n",
    "    of loss and grads.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:  matrix\n",
    "        an input data of shape (n, d). Each x[i] is a training sample.\n",
    "    y:  vector\n",
    "        a vector of training labels. Each y[i] is an integer in the range\n",
    "        0 <= y[i] < c. y[i] is the label for x[i]. If it is passed then we\n",
    "        return the loss and gradients.\n",
    "    regular: float\n",
    "        regularization strength.\n",
    "    enable_margin: Bool\n",
    "        enable to use soft-margin softmax\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss:\n",
    "        Loss (data loss and regularization loss) for this batch of training\n",
    "        samples.\n",
    "    grads:\n",
    "        Dictionary mapping parameter names to gradients of those parameters with\n",
    "        respect to the loss function; has the same keys as self.params.\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    n, d   = x.shape # input dimensions\n",
    "    w1, b1 = self.params['w1'], self.params['b1']\n",
    "    w2, b2 = self.params['w2'], self.params['b2']\n",
    "    h1     = None  # the activation after the first linear layer\n",
    "    y1, y2 = None, None  # outputs from the first & second linear layers\n",
    "\n",
    "    # Compute the forward pass\n",
    "    out, cache = self.forward_pass(x,w1,b1,w2,b2)\n",
    "    y2       = out\n",
    "    (y1, h1) = cache\n",
    "\n",
    "    # If the targets are not given then jump out, we're done\n",
    "    if y is None:\n",
    "      return y2\n",
    "\n",
    "    # Compute the loss\n",
    "    loss, dY2_dLoss = self.softmax_loss(y2, y)\n",
    "\n",
    "    # Compute the backward pass\n",
    "    grads = self.backward_pass(dY2_dLoss, x, w1, y1, h1, w2)\n",
    "\n",
    "    #############################################################################\n",
    "    # PLACE YOUR CODE HERE (REGULARIZATION)                                     #\n",
    "    #############################################################################\n",
    "    # TODO: Implement weight regularization\n",
    "    #loss +=\n",
    "\n",
    "\n",
    "    #add regularization effect to the gradient terms\n",
    "    #grads['w2'] +=\n",
    "    #grads['w1'] +=\n",
    "\n",
    "    #  END OF YOUR CODE\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "  def train(self, x, y, x_v, y_v,\n",
    "            eta=1e-3, lamdba=0.95,\n",
    "            regular=1e-5, num_iters=50,\n",
    "            batch_size=100, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this neural network using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (n, d) giving training data.\n",
    "    - y: A numpy array f shape (n,) giving training labels; y[i] = C means that\n",
    "      x[i] has label C, where 0 <= C < c.\n",
    "    - x_v: A numpy array of shape (n_v, d) giving validation data.\n",
    "    - y_v: A numpy array of shape (n_v,) giving validation labels.\n",
    "    - eta: Scalar giving learning rate for optimization.\n",
    "    - lamdba: Scalar giving factor used to decay the learning rate\n",
    "      after each epoch.\n",
    "    - regular: Scalar giving regularization strength.\n",
    "    - num_iters: Number of steps to take when optimizing.\n",
    "    - batch_size: Number of training examples to use per step.\n",
    "    - verbose: boolean; if true print progress during optimization.\n",
    "    \"\"\"\n",
    "    num_train = x.shape[0]\n",
    "    iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "    # Use Stochastic Gradient Descent (SGD) to optimize the parameters in\n",
    "    # self.model\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for it in range(num_iters):\n",
    "      x_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      #########################################################################\n",
    "      # PLACE YOUR CODE HERE                                                  #\n",
    "      #########################################################################\n",
    "      # TODO: Create a random minibatch of training data and labels, storing  #\n",
    "      # them in x_batch and y_batch respectively.                             #\n",
    "\n",
    "      random_sample = np.random.choice(num_train,batch_size)\n",
    "      x_batch = x[random_sample]\n",
    "      y_batch = y[random_sample]\n",
    "\n",
    "      #  END OF YOUR CODE\n",
    "      #########################################################################\n",
    "\n",
    "      # Compute loss and gradients using the current minibatch\n",
    "      loss, grads = self.loss(x_batch, y=y_batch, regular=regular)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      #########################################################################\n",
    "      # PLACE YOUR CODE HERE                                                  #\n",
    "      #########################################################################\n",
    "      # TODO: Update the parameters of the network stored in self.params by   #\n",
    "      # using the gradients in the grads dictionary. For that, use stochastic #\n",
    "      # gradient descent.                                                     #\n",
    "\n",
    "      #self.params['w1'] +=\n",
    "      #self.params['w2'] +=\n",
    "      #self.params['b1'] +=\n",
    "      #self.params['b2'] +=\n",
    "\n",
    "      #  END OF YOUR CODE\n",
    "      #########################################################################\n",
    "\n",
    "      #########################################################################\n",
    "      # PLACE YOUR CODE HERE                                                  #\n",
    "      #########################################################################\n",
    "      # For printing out validation acc\n",
    "      if verbose and it % 100 == 0:\n",
    "        # get validataion loss\n",
    "        # print out\n",
    "        pass\n",
    "\n",
    "\n",
    "      #  END OF YOUR CODE\n",
    "      #########################################################################\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print('The #iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "      # Every epoch, check train and val accuracy and decay learning rate.\n",
    "      if it % iterations_per_epoch == 0:\n",
    "        # Check accuracy\n",
    "        train_acc = (self.predict(x_batch) == y_batch).mean()\n",
    "        val_acc = (self.predict(x_v) == y_v).mean()\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "        # Decay learning rate\n",
    "        eta *= lamdba\n",
    "\n",
    "    return {\n",
    "      'loss_history': loss_history,\n",
    "      'train_acc_history': train_acc_history,\n",
    "      'val_acc_history': val_acc_history,\n",
    "    }\n",
    "\n",
    "  def predict(self, x):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this MLP network to predict labels for\n",
    "    data points. For each data point we predict labels for each of the C\n",
    "    classes, and assign each data point to the class with the highest label\n",
    "    score.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (n, d) giving n d-dimensional data points to\n",
    "      classify.\n",
    "\n",
    "    Returns:\n",
    "    - y_pr: A numpy array of shape (n,) giving predicted labels for each of\n",
    "      the elements of x. For all i, y_pred[i] = c means that x[i] is predicted\n",
    "      to have class C, where 0 <= C < c.\n",
    "    \"\"\"\n",
    "    y_pr = None\n",
    "\n",
    "    ###########################################################################\n",
    "    # PLACE YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the predict function                                    #\n",
    "    #out, _ =\n",
    "\n",
    "\n",
    "\n",
    "    #y_pr =\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "    ###########################################################################\n",
    "\n",
    "    return y_pr\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected using ReLU\n",
      "(200, 10)\n",
      "(200,)\n",
      "(200, 10)\n",
      "(200, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (200,) (200,10) ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m net_mlp \u001B[38;5;241m=\u001B[39m MLP(input_size, hidden_size, num_classes, activation)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Train the network\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m stats \u001B[38;5;241m=\u001B[39m \u001B[43mnet_mlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnum_iters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m            \u001B[49m\u001B[43meta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlamdba\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.95\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m            \u001B[49m\u001B[43mregular\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Predict on the validation set\u001B[39;00m\n\u001B[1;32m     15\u001B[0m val_acc \u001B[38;5;241m=\u001B[39m (net_mlp\u001B[38;5;241m.\u001B[39mpredict(X_val) \u001B[38;5;241m==\u001B[39m y_val)\u001B[38;5;241m.\u001B[39mmean()\n",
      "Cell \u001B[0;32mIn[28], line 316\u001B[0m, in \u001B[0;36mMLP.train\u001B[0;34m(self, x, y, x_v, y_v, eta, lamdba, regular, num_iters, batch_size, verbose)\u001B[0m\n\u001B[1;32m    310\u001B[0m y_batch \u001B[38;5;241m=\u001B[39m y[random_sample]\n\u001B[1;32m    312\u001B[0m \u001B[38;5;66;03m#  END OF YOUR CODE\u001B[39;00m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;66;03m#########################################################################\u001B[39;00m\n\u001B[1;32m    314\u001B[0m \n\u001B[1;32m    315\u001B[0m \u001B[38;5;66;03m# Compute loss and gradients using the current minibatch\u001B[39;00m\n\u001B[0;32m--> 316\u001B[0m loss, grads \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mregular\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mregular\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    317\u001B[0m loss_history\u001B[38;5;241m.\u001B[39mappend(loss)\n\u001B[1;32m    319\u001B[0m \u001B[38;5;66;03m#########################################################################\u001B[39;00m\n\u001B[1;32m    320\u001B[0m \u001B[38;5;66;03m# PLACE YOUR CODE HERE                                                  #\u001B[39;00m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;66;03m#########################################################################\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    336\u001B[0m \u001B[38;5;66;03m#########################################################################\u001B[39;00m\n\u001B[1;32m    337\u001B[0m \u001B[38;5;66;03m# For printing out validation acc\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[28], line 245\u001B[0m, in \u001B[0;36mMLP.loss\u001B[0;34m(self, x, y, regular, enable_margin)\u001B[0m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28mprint\u001B[39m(y2\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m    244\u001B[0m \u001B[38;5;28mprint\u001B[39m(y\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m--> 245\u001B[0m loss, dY2_dLoss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoftmax_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43my2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;66;03m# Compute the backward pass\u001B[39;00m\n\u001B[1;32m    248\u001B[0m grads \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackward_pass(dY2_dLoss, x, w1, y1, h1, w2)\n",
      "Cell \u001B[0;32mIn[28], line 129\u001B[0m, in \u001B[0;36mMLP.softmax_loss\u001B[0;34m(self, x, y)\u001B[0m\n\u001B[1;32m    126\u001B[0m epsilon \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1e-15\u001B[39m\n\u001B[1;32m    127\u001B[0m x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mclip(x, epsilon, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m epsilon)\n\u001B[0;32m--> 129\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39msum(\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(x)\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28mprint\u001B[39m(loss)\n\u001B[1;32m    132\u001B[0m dx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: operands could not be broadcast together with shapes (200,) (200,10) "
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 64\n",
    "num_classes = 10\n",
    "activation = 'ReLU' # Select one in [ReLU, LeakyReLU, SWISH, 'SELU']\n",
    "net_mlp = MLP(input_size, hidden_size, num_classes, activation)\n",
    "\n",
    "# Train the network\n",
    "stats = net_mlp.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            eta=1e-3, lamdba=0.95,\n",
    "            regular=1.0, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net_mlp.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
